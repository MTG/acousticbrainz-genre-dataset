%%%% Proceedings format for most of ACM conferences (with the exceptions listed below) and all ICPS volumes.
\documentclass[sigconf]{acmart-me}
%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% Proceedings format for SIGPLAN conferences
% \documentclass[sigplan, anonymous, review]{acmart}

%%%% Proceedings format for SIGCHI conferences
% \documentclass[sigchi, review]{acmart}

%%%% To use the SIGCHI extended abstract template, please visit
% https://www.overleaf.com/read/zzzfqvkmrfzn

\usepackage{booktabs} % For formal tables
\usepackage{url}
\usepackage{color}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{soul}
\hyphenation{Media-Eval}

%\renewcommand*{\UrlFont}{\ttfamily\smaller\relax}

% Compress lists to single spacing
\newcommand{\compresslist}{%
    \setlength{\itemsep}{1pt}%
    \setlength{\parskip}{0pt}%
    \setlength{\parsep}{0pt}%
}

% removing natural indent in itemize
\setlist[itemize]{leftmargin=*}


% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{}

% ISBN
\acmISBN{}

%Conference
\acmConference[MediaEval'17]{Multimedia Evaluation Workshop}{13-15 September 2017}{Dublin, Ireland}
\acmYear{}
\copyrightyear{}

\acmPrice{}


\begin{document}
\title{The MediaEval 2017 AcousticBrainz Genre Task:\\Content-based Music Genre Recognition from Multiple Sources}
%\titlenote{Produces the permission block, and
%  copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
 % \texttt{acmart.pdf} document}


%%If you want to use multi-column authors that's fine. Comment out the next lines, and then uncomment below.
\author{Dmitry Bogdanov\textsuperscript{1}, Alastair Porter\textsuperscript{1}, Juli√°n Urbano\textsuperscript{2}, Hendrik Schreiber\textsuperscript{3}}
\affiliation{\textsuperscript{1}Music Technology Group, Universitat Pompeu Fabra, Spain\\ \textsuperscript{2}Multimedia Computing Group, Delft University of Technology, Netherlands \\ \textsuperscript{3}tagtraum industries incorporated\\ }
\email{dmitry.bogdanov@upf.edu, alastair.porter@upf.edu, urbano.julian@gmail.com, hs@tagtraum.com}

%\author{G.K.M. Tobin}
%%\authornote{The secretary disavows any knowledge of this author's actions.}
%\affiliation{Institute for Clarity in Documentation, Ohio, USA}
%\email{webmaster@marysville-ohio.com}
%
%\author{Lars Th{\o}rv{\"a}ld}
%%\authornote{This author is the
%%  one who did all the really hard work.}
%\affiliation{The Th{\o}rv{\"a}ld Group, Iceland}
%\email{larst@affiliation.org}
%
%\author{Lawrence P. Leipuner}
%\affiliation{Brookhaven Labs, France}
%\email{lleipuner@researchlabs.org}
%
%\author{Sean Fogarty}
%\affiliation{A Research Institute, Germany}
%\email{fogartys@amesres.org}
%
%\author{Charles Palmer}
%\affiliation{Palmer Research Laboratories, Texas, USA}
%\email{cpalmer@prl.com}
%
%\author{John Smith}
%\affiliation{The Th{\o}rv{\"a}ld Group, Iceland}
%\email{jsmith@affiliation.org}

\renewcommand{\shortauthors}{D. Bogdanov et al.}
\renewcommand{\shorttitle}{AcousticBrainz Genre Task}

\begin{abstract}

This paper provides an overview of the AcousticBrainz Genre Task organized as
part of the MediaEval 2017 Benchmarking Initiative for Multimedia Evaluation.
The task is focused on content-based music genre recognition using genre
annotations from multiple sources and large-scale music features data available
in the AcousticBrainz database. The goal of our task is to explore how the same
music pieces can be annotated differently by different communities following
different genre taxonomies, and how this should be addressed by content-based
genre recognition systems. We present the task challenges, the employed
ground-truth information and datasets, and the evaluation methodology.

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010553.10010554</concept_id>
%  <concept_desc>Computer systems organization~Robotics</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>10003033.10003083.10003095</concept_id>
%  <concept_desc>Networks~Network reliability</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>
%\end{CCSXML}
%
%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}
%
%% We no longer use \terms command
%%\terms{Theory}
%
%\keywords{ACM proceedings, \LaTeX, text tagging}

%% Used in some conference proceedings e.g. sigplan and sigchi
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{This is a teaser}
%   \label{fig:teaser}
% \end{teaserfigure}

\maketitle

\section{Introduction}

Content-based music genre recognition is a popular task
in Music Information Retrieval research~\cite{Sturm2014state}. The goal is to build systems able to predict genre and subgenre of unknown music recordings (tracks or songs)
using music features of those recordings automatically computed from audio.
%Achieving success in the task therefore relies on building efficient machine
%learning and audio signal processing algorithms.
%\hl{Although this task is commonly addressed in the MIR research, we realize that it is often oversimplified while it should be considered in more complexity resembling real-world use cases.}
%In particular,
Such research can be supported
by our recent developments in the context of the
AcousticBrainz~\cite{acousticbrainz} %\footnote{\url{https://acousticbrainz.org}}
project, which facilitates
access to large datasets of music features~\cite{porter2015acousticbrainz} and
metadata~\cite{porter2016mining}. AcousticBrainz is a community database
containing music features extracted from audio files. Users who contribute to the
project run software on their computers to process their personal audio
collections and submit music features to the AcousticBrainz database.
Based on these features, additional
metadata including genres can then be mined for recordings in the database.

We propose a new genre recognition task using datasets based on AcousticBrainz
for MediaEval 2017. This task is different from a typical genre recognition task
in the following ways:

\begin{itemize}
\item It allows us to explore how the same music can be annotated differently by different
communities who follow different genre taxonomies, and how this can be
addressed when developing and evaluating genre recognition systems.

\item Genre recognition is often treated as a single category classification
problem. %, which is not necessarily the way it should be. O
Our data is intrinsically multi-label, so we propose to treat genre recognition as a
multi-label classification problem.

\item Previous research typically used a small number of broad genre categories.
In contrast, we consider more specific genres and subgenres. Our
data contains hundreds of subgenres.

\item We provide information about the hierarchy of genres and
subgenres within each annotation source. Systems can take advantage of
this knowledge.

\item MIR research is often performed on small music collections. We
provide a very large dataset with two million recordings annotated with genres
and subgenres. However, we only provide precomputed features, not audio.


\end{itemize}


%Bringing the AcousticBrainz Genre Task to MediaEval, our goal is to benefit from
%contributions and expertise of a broader machine learning and multimedia
%retrieval community.


\section{Task Description}

The task invites participants to predict the genre and subgenre of unknown music
recordings given automatically computed music features of those recordings.
We provide four datasets of such music features taken from the AcousticBrainz
database~\cite{porter2015acousticbrainz} together with four different ground
truths created using
four different music metadata websites as sources. Their genre taxonomies vary in
class spaces, specificity and breadth. Each source has its own definition for
its genre labels, i.e.\ the same label may carry a different meaning when used
by another source.
Most importantly, annotations in each source are multi-label: there may be multiple genre and
subgenre annotations for the same music recording. It is guaranteed that each
recording has at least one genre label, while subgenres are not always present.


Participants must train model(s) using the provided development sets and then predict
genre and subgenre labels for the test sets.
%We provide a dataset (a training and test set) for every genre ground truth.
%In total, participants are given four training datasets and all proposed
%models are evaluated on four test datasets.
%The goal is to create a system that
%uses provided music features as an input and predicts genre and subgenre labels,
%using the genre taxonomy of each ground truth.
The task includes two subtasks:

\begin{itemize}

\item \textbf{Subtask 1: Single-source Classification}. This subtask explores
conventional systems, each one trained on a single dataset. Participants submit predictions for the test set of each dataset separately, using their
respective class spaces (genres and subgenres). These predictions will be
produced by a separate system for each dataset, that has been trained without any information
from the other sources. This subtask will serve as a baseline for Subtask 2.

\item \textbf{Subtask 2: Multi-source Classification}. This subtask explores
the combination of several ground-truth sources to create a single classification system.
We use the same four test sets. Participants submit predictions for
each test set separately, again following each corresponding genre class space.
These predictions may be produced by a single system for all datasets or by one
system for each dataset. Participants are free to make their own decision about
how to combine the training data from all sources.

\end{itemize}

%\begin{itemize}
%
%\item Subtask 1: consider each genre ground truth individually to generate
%predictions for the four test datasets. This subtask will serve as a baseline
%for Subtask 2.
%
%\item  Subtask 2: combine genre ground truths together to generate predictions
%for the four test datasets.
%
%\end{itemize}


\section{Data}

\subsection{Genre Annotations}

We provide four datasets containing genre and subgenre annotations extracted
from four different online metadata sources:

\begin{itemize}

\item \textbf{AllMusic}~\cite{allmusic} %\footnote{\url{https://allmusic.com}}
and \textbf{Discogs}~\cite{discogs} %\footnote{\url{https://discogs.com}}
are based on editorial metadata databases maintained by music experts and enthusiasts. These sources contain explicit genre/subgenre annotations of music releases (albums) following a predefined genre taxonomy. To build the datasets we assumed that release-level annotations
correspond to all recordings in AcousticBrainz for that release.

\item \textbf{Lastfm}~\cite{lastfm} %\footnote{\url{https://last.fm}}
is based on a collaborative music tagging platform with large amounts of genre labels provided by its users for music recordings.
\textbf{Tagtraum}~\cite{tagtraum} %\footnote{\url{http://www.tagtraum.com}}
is similarly based on genre labels
collected from users of the music tagging application beaTunes~\cite{beatunes}. %\footnote{\url{https://www.beatunes.com}}
We have automatically inferred a genre/subgenre taxonomy and annotations from these labels following the algorithm proposed in~\cite{schreiber_improving_2015} and a manual post-processing.

\end{itemize}
We provide information about genre/subgenre tree hierarchies for every ground
truth.\footnote{
The resulting genre metadata is licensed under CC BY-NC-SA4.0
license, except
for data extracted from the AllMusic database which is released for
non-commercial scientific research purposes only. Any publication of results based
on the data extracts of the AllMusic database must cite AllMusic as the source
of the data.
}
%The resulting genre metadata is licensed under CC BY-NC-SA4.0
%license,\footnote{ %Creative Commons Attribution-NonCommercial-ShareAlike 4.0
%International, \url{https://creativecommons.org/licenses/by-nc-sa/4.0/}} except
%for data extracted from the AllMusic database which is released for non-
%commercial scientific research purposes only. Any publication of results based
%on the data extracts of the AllMusic database must cite AllMusic as the source
%of the data.


\subsection{Music Features}

We provide music features precomputed from audio for every music recording. All
features are taken from the AcousticBrainz database and were
extracted from audio using Essentia, an open-source library for music audio
analysis~\cite{bogdanov2013essentia}. The provided features
%are grouped into categories (low-level, rhythm, and tonal) and
are explained online.\footnote{\url{http://essentia.upf.edu/documentation/streaming_extractor_music.html}} Only statistical
characterization of time frames is provided (bag of features), that is, no
frame-level data is available.

\subsection{Development and Test Datasets}

In total we provide four development (training+validation) and four test datasets associated with the
four genre ground truths. They were created by a random split of
the full data ensuring that:
\begin{itemize}
\item no recordings in the test sets are present in any of the development sets;
\item no recordings in the test sets are from the same release groups (e.g., albums, singles, EPs) present in the development sets;
\item the same genre and subgenre labels are present in both development and test sets for each ground truth;
\item genre and subgenre labels are represented by at least 40 and 20 recordings from 6 and 3 release groups in development and test sets, respectively.
\end{itemize}

\begin{table}[t]
  \caption{Overview of the development datasets.}
  \label{table:train_stats}
  {\small\begin{tabular}{lcccc}
  \toprule
    Dataset & AllMusic & Discogs & Lastfm & Tagtraum \\
  \midrule
    Type & Explicit & Explicit & Tags & Tags \\
    Annotation level & Release & Release & Track & Track \\
  \midrule
    Recordings & 1,353,213 & 904,944 & 566,710 & 486,740 \\
    Release groups & 163,654 & 118,475 & 115,161 & 69,025 \\
  \midrule
    Genres & 21 & 15 & 30 & 31 \\
    Subgenres & 745 & 300 & 297 & 265 \\
    Genres/track & 1.33 & 1.37 & 1.14 & 1.13 \\
    Subgenres/track & 3.15 & 1.69 & 1.28 & 1.72 \\
  \bottomrule
\end{tabular}}
\end{table}

The approximate split ratios are 70\% for training and 15\% for testing (another 15\% was reserved for future evaluation purposes).
Table~\ref{table:train_stats} provides an overview of the resulting development
sets. Details on the genre/subgenre taxonomy and their distribution in the development sets in terms of number of recordings and
release groups %for all four training sets
are reported online.\footnote{\url{https://multimediaeval.github.io/2017-AcousticBrainz-Genre-Task/data_stats/}}
Recordings are partially intersected (annotated by all four ground truths) in the development and test sets. The full intersection of all
development sets contains 247,716 recording, while the intersection of the two
largest %development
sets, AllMusic and Discogs, contains 831,744 recordings.

All data are published in JSON and TSV formats; details about format are available
online.\footnote{\url{https://multimediaeval.github.io/2017-AcousticBrainz-Genre-Task/data/}}
Each recording in the development sets is
identified by a %recording
MusicBrainz ID (MBID)\footnote{\url{https://musicbrainz.org/doc/MusicBrainz_Identifier}}, which can be used by participants to gather related data.
%All identifiers in the test datasets are anonymized for evaluation purposes.
Importantly, our split allows to avoid the ``album
effect''~\cite{flexer2009album}, which leads to a potential overestimation of
the performance of a system when a test set contains recordings from
the same albums as the development set.
%For this reason we also included information
%about release groups of each recording in the development sets to help participants
%to avoid this effect when developing their systems.
The development sets additionally
include information about release groups of each recording,
%(release group MBID)
which may be useful for participants in order to avoid this effect when
developing their systems.
Partitioning scripts were provided to create training-validation splits ensuring these characteristics in the data.

%The development datasets additionally include information about release groups
%to which the recordings belong (release group MBID). This information may be
%useful for participants in order to avoid the ``album
%effect''~\cite{flexer2009album}, which consists in potential overestimation of
%the performance of a classifier when a test set contains music recordings from
%the same albums as the development set. Our random split of recordings into
%development and test datasets ensures that such an effect does not occur. The
%approximate split ratio is 70\% to 15\% (another 15\% are reserved for further
%evaluation purposes).


\section{Submissions and Evaluation}

Participants are expected to submit predictions for both subtasks. %If they only
%want to work on the first subtask, they should submit the same predictions for
%the second subtask.
We allow a maximum of five evaluation runs, each including both
subtasks, and reporting whether they used the whole development
dataset or only parts for every submission.

%\section{Evaluation Methodology}

The evaluation is carried out for each dataset separately. We do not use hierarchical measures because the hierarchies in the Lastfm and Tagtraum datasets are not explicit. Instead, we
compute precision, recall and F-score at different levels:

\begin{itemize}
\item Per recording, all labels.
\item Per recording, only genre labels.
\item Per recording, only subgenre labels.
\item Per label, all recordings.
\item Per genre label, all recordings.
\item Per subgenre label, all recordings.
\end{itemize}

The ground truth does not necessarily contain subgenre annotations for some
recordings, so we only considered recordings containing subgenres for
the evaluation at the subgenre level. An example can be found online in the
summaries of random baselines.\footnote{\url{https://multimediaeval.github.io/2017-AcousticBrainz-Genre-Task/baseline/}}
We also provided evaluation scripts for development purposes.

\section{Conclusions}
Bringing the AcousticBrainz Genre Task to MediaEval we hope to benefit from
contributions and expertise of a broader machine learning and multimedia retrieval
community. We refer to the MediaEval 2017 proceedings for further details on the
methods and results of teams participating in the task.

%\begin{figure}
%\includegraphics{fly}
%\caption{A sample black and white graphic.}
%\end{figure}

%\begin{figure}
%\includegraphics[height=1in, width=1in]{fly}
%\caption{A sample black and white graphic
%that has been resized with the \texttt{includegraphics} command.}
%\end{figure}


%\begin{figure*}
%\includegraphics{flies}
%\caption{A sample black and white graphic
%that needs to span two columns of text.}
%\end{figure*}
%
%
%\begin{figure}
%\includegraphics[height=1in, width=1in]{rosette}
%\caption{A sample black and white graphic that has
%been resized with the \texttt{includegraphics} command.}
%\end{figure}


\begin{acks}

We thank all contributors to AcousticBrainz. This research has
received funding from the European Union's Horizon 2020 research and innovation
programme under grant agreement No 688382 (AudioCommons).
We also thank tagtraum industries for providing the Tagtraum
genre annotations.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\def\bibfont{\small} % comment this line for a smaller fontsize
\bibliography{acousticbrainz-genre-task-me17}

\end{document}
