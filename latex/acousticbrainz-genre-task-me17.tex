%%%% Proceedings format for most of ACM conferences (with the exceptions listed below) and all ICPS volumes.
\documentclass[sigconf]{acmart-me}
%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% Proceedings format for SIGPLAN conferences 
% \documentclass[sigplan, anonymous, review]{acmart}

%%%% Proceedings format for SIGCHI conferences
% \documentclass[sigchi, review]{acmart}

%%%% To use the SIGCHI extended abstract template, please visit
% https://www.overleaf.com/read/zzzfqvkmrfzn

\usepackage{booktabs} % For formal tables
\usepackage{url}
\usepackage{color}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{soul}
\hyphenation{Media-Eval}

%\renewcommand*{\UrlFont}{\ttfamily\smaller\relax}

% Compress lists to single spacing
\newcommand{\compresslist}{%
    \setlength{\itemsep}{1pt}%
    \setlength{\parskip}{0pt}%
    \setlength{\parsep}{0pt}%
}

% removing natural indent in itemize
\setlist[itemize]{leftmargin=*}


% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{}

% ISBN
\acmISBN{}

%Conference
\acmConference[MediaEval'17]{Multimedia Evaluation Workshop}{13-15 September 2017}{Dublin, Ireland} 
\acmYear{}
\copyrightyear{}

\acmPrice{}


\begin{document}
\title{The MediaEval 2017 AcousticBrainz Genre Task: Content-based music genre recognition from multiple sources}
%\titlenote{Produces the permission block, and
%  copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
 % \texttt{acmart.pdf} document}


%%If you want to use multi-column authors that's fine. Comment out the next lines, and then uncomment below.
\author{Dmitry Bogdanov\textsuperscript{1}, Alastair Porter\textsuperscript{1}, Juli√°n Urbano\textsuperscript{2}, Hendrik Schreiber\textsuperscript{3}}
\affiliation{\textsuperscript{1}Music Technology Group, Universitat Pompeu Fabra, Spain\\ \textsuperscript{2}Multimedia Computing Group, Delft University of Technology, Netherlands \\ \textsuperscript{3}Tagtraum Industries Incorporated\\ }
\email{dmitry.bogdanov@upf.edu, alastair.porter@upf.edu, j.urbano@tudelft.nl, hs@tagtraum.com}

%\author{G.K.M. Tobin}
%%\authornote{The secretary disavows any knowledge of this author's actions.}
%\affiliation{Institute for Clarity in Documentation, Ohio, USA}
%\email{webmaster@marysville-ohio.com}
%
%\author{Lars Th{\o}rv{\"a}ld}
%%\authornote{This author is the
%%  one who did all the really hard work.}
%\affiliation{The Th{\o}rv{\"a}ld Group, Iceland}
%\email{larst@affiliation.org}
%
%\author{Lawrence P. Leipuner}
%\affiliation{Brookhaven Labs, France}
%\email{lleipuner@researchlabs.org}
%
%\author{Sean Fogarty}
%\affiliation{A Research Institute, Germany}
%\email{fogartys@amesres.org}
%
%\author{Charles Palmer}
%\affiliation{Palmer Research Laboratories, Texas, USA}
%\email{cpalmer@prl.com}
%
%\author{John Smith}
%\affiliation{The Th{\o}rv{\"a}ld Group, Iceland}
%\email{jsmith@affiliation.org}

\renewcommand{\shortauthors}{D. Bogdanov et al.}
\renewcommand{\shorttitle}{AcousticBrainz Genre Task}

\begin{abstract}  

This paper provides an overview of the AcousticBrainz Genre Task organized as
part of the MediaEval 2017 Benchmarking Initiative for Multimedia Evaluation.
The task is focused on content-based music genre recognition using genre
annotations from multiple sources and large-scale music features data available
in the AcousticBrainz database. The goal of our task is to explore how the same
music pieces can be annotated differently by different communities following
different genre taxonomies, and how this should be addressed by content-based
genre recognition systems. We present the task challenges, the employed 
ground-truth information and datasets, and the evaluation methodology.

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010553.10010554</concept_id>
%  <concept_desc>Computer systems organization~Robotics</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>10003033.10003083.10003095</concept_id>
%  <concept_desc>Networks~Network reliability</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>  
%\end{CCSXML}
%
%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}
%
%% We no longer use \terms command
%%\terms{Theory}
%
%\keywords{ACM proceedings, \LaTeX, text tagging}

%% Used in some conference proceedings e.g. sigplan and sigchi
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{This is a teaser}
%   \label{fig:teaser}
% \end{teaserfigure}

\maketitle

\section{Introduction}  Content-based music genre recognition is a popular task
within the Music Information Retrieval (MIR)
community~\cite{sturm2013classification}. The goal is to build systems that are
able to predict genre and subgenre of unknown music recordings (tracks or songs)
using music features of those recordings automatically computed from audio.
%Achieving success in the task therefore relies on building efficient machine
%learning and audio signal processing algorithms.
\hl{Although this task is commonly addressed in the MIR research, we realize that it
is often oversimplified while it should be considered in more complexity
resembling real-world use cases.} 
%In particular, 
Such research can be supported
by our recent developments in the context of the
AcousticBrainz\footnote{\url{https://acousticbrainz.org}} project facilitating
access to large datasets of music features~\cite{porter2015acousticbrainz} and
metadata~\cite{porter2016mining}. AcousticBrainz is a community database
containing MIR features extracted from audio files. Users who contribute to the
project run software on their computers to analyze their personal audio
collections and submit the analysis to the AcousticBrainz database. Additional
metadata including genres can then be mined for recordings in the database.

We propose a new genre recognition task using datasets based on AcousticBrainz
for MediaEval 2017. This task is different from a typical genre recognition task
in the following:

\begin{itemize}

\item There are different genre taxonomies and people may not always agree on
the meaning of genres. Genres labels are probably subjective categories. We want
to explore how the same music can be annotated differently by different
communities following different genre taxonomies, and how this should be
addressed by genre recognition systems.

\item Typically research is done on a small number of broad genre categories.
In contrast, we propose to consider more specific genres and subgenres and our
data contains hundreds of subgenres.

\item Genre recognition is often treated as a single category classification
problem. %, which is not necessarily the way it should be. O
Our genre data is
intrinsically multi-label and so we propose to treat genre recognition as a
multi-label classification problem.

\item Typically research is done on small music collections. Instead, we
provide a very large dataset counting two million recordings annotated by genres
and subgenres. The downside is that we are not able to provide audio, but only
precomputed bags of features.

\item Finally, we provide information about the hierarchy of genres and
subgenres within each genre annotation source. Systems can take advantage of
this knowledge.

\end{itemize} 

%Bringing the AcousticBrainz Genre Task to MediaEval, our goal is to benefit from
%contributions and expertise of a broader machine learning and multimedia
%retrieval community.


\section{Task Description} 

The task invites participants to predict genre and subgenre of unknown music
recordings (tracks) given automatically computed features of those recordings.
We provide a training set of such music features taken from the AcousticBrainz
database~\cite{porter2015acousticbrainz} together with four different ground
truths of genre and subgenre labels. These ground truths were created using as a
source four different music metadata websites. Their genre taxonomies vary in
class spaces, specificity and breadth. Each source has its own definition for
its genre labels meaning that these labels may be different between sources.
Importantly, annotations in each source are multi-label: there may be multiple genre and
subgenre annotations for the same music recording. It is guaranteed that each
recording has at least one genre label, while subgenres are not always present.


Participants must train model(s) using this data and then generate predictions
of genre and subgenre labels for a test set
%We provide a dataset (a development and test set) for every genre ground truth.
%In total, participants are given four development datasets and all proposed
%models are evaluated on four test datasets. 
%The goal is to create a system that
%uses provided music features as an input and predicts genre and subgenre labels,
following genre taxonomy of each ground truth. The task includes two subtasks:

\begin{itemize}

\item \textbf{Subtask 1: Single-source Classification}. This subtask explores
conventional systems, each one trained on a single dataset. Participants will
submit predictions for the test set of each dataset separately, following their
respective class spaces (genres and subgenres). These predictions will be
produced by a separate system for each dataset, trained without any information
from the other sources. This subtask will serve as a baseline for Subtask 2.

\item \textbf{Subtask 2: Multi-source Classification}. This subtask explores
how to combine several ground-truth sources to create a classification system.
We will use the same four test sets. Participants will submit predictions for
each test set separately, again following each corresponding genre class space.
These predictions may be produced by a single system for all datasets or by one
system for each dataset. Participants are free to make their own decision about
how to combine the training data from all sources.

\end{itemize}

%\begin{itemize}
%
%\item Subtask 1: consider each genre ground truth individually to generate
%predictions for the four test datasets. This subtask will serve as a baseline
%for Subtask 2.
%
%\item  Subtask 2: combine genre ground truths together to generate predictions
%for the four test datasets.
%
%\end{itemize}


\section{Data}

\subsection{Genre Annotations}

We provide four datasets containing genre and subgenre annotations extracted
from four different online metadata sources:
\footnote{
The resulting genre metadata is licensed under CC BY-NC-SA4.0
license, except
for data extracted from the AllMusic database which is released for 
non-commercial scientific research purposes only. Any publication of results based
on the data extracts of the AllMusic database must cite AllMusic as the source
of the data.
}  

\begin{itemize}

\item \textbf{AllMusic}\footnote{\url{https://allmusic.com}} and \textbf{Discogs}\footnote{\url{https://discogs.com}} are based on editorial metadata databases maintained by music experts and enthusiasts. These sources contain explicit genre/subgenre annotations of music releases (albums) following a predefined genre namespace and taxonomy. We propagated release-level annotations to recordings (tracks) in AcousticBrainz present on those releases to build the datasets.

\item \textbf{Lastfm}\footnote{\url{https://last.fm}} and \textbf{Tagtraum}\footnote{\url{https://tagtraum.com}} are based on collaborative music tagging platforms with large amounts of genre labels provided by their users for music recordings (tracks). We have automatically inferred a genre/subgenre taxonomy and annotations from these labels following the algorithm proposed in~\cite{schreiber_improving_2015} and a manual post-processing.

\end{itemize}
We provide information about genre/subgenre tree hierarchies for every ground
truth. 
%The resulting genre metadata is licensed under CC BY-NC-SA4.0
%license,\footnote{ %Creative Commons Attribution-NonCommercial-ShareAlike 4.0
%International, \url{https://creativecommons.org/licenses/by-nc-sa/4.0/}} except
%for data extracted from the AllMusic database which is released for non-
%commercial scientific research purposes only. Any publication of results based
%on the data extracts of the AllMusic database must cite AllMusic as the source
%of the data.


\subsection{Music Features} 

We provide music features precomputed from audio for every music recording. All
features are taken from the AcousticBrainz database and were
extracted from audio using Essentia, an open-source library for music audio
analysis~\cite{bogdanov2013essentia}. The provided features 
%are grouped into categories (low-level, rhythm, and tonal) and 
are explained online.
\footnote{\url{http://essentia.upf.edu/documentation/streaming_extractor_music.html}} Only statistical
characterization of time frames is provided (bag of features), that is, no
frame-level data is available.

\subsection{Development and Test Datasets}  

In total we provide four development and four test datasets associated with the
four genre ground truths. They were created by a random split of 
the full data ensuring that:
\begin{itemize}
\item no recordings from test sets are also present in any of the development sets;
\item no recordings from test sets are from the same release groups (e.g., albums, singles, EPs) as the development sets; 
\item the same genre and subgenre labels are present in both development and test sets for each ground truth;
\item genre and subgenre labels are represented by at least 40 and 20 recordings from 6 and 3 release groups in development and test sets, respectively.
\end{itemize}
The approximate split ratio is 70\% to 15\% of the total number of recordings
(another 15\% were reserved for further evaluation purposes).
Table~\ref{table:train_stats} provides an overview of the resulting development
sets. Genre/subgenre taxonomy and their distribution in the development sets in terms of number of recordings and
release groups %for all four development sets 
are reported online.\footnote{\url{https://multimediaeval.github.io/2017-AcousticBrainz-Genre-Task/data_stats/}} 
Recordings are partially intersected in all four
development sets as well as in the test sets. The full intersection of all
development sets (recordings annotated by all four ground truths) contains 247716 recordings while the intersection of the two
largest %development 
sets, AllMusic and Discogs, contains 831744 recordings.

The details on the format of all distributed data are available
online.\footnote{\url{https://multimediaeval.github.io/2017-AcousticBrainz-Genre-Task/data/}} 
Each recording (a track or song) in the development sets is
identified with a %recording 
MusicBrainz identifier (MBID).\footnote{\url{https://musicbrainz.org/doc/MusicBrainz_Identifier}} 
%All identifiers in the test datasets are anonymized for evaluation purposes.
Importantly, our split allows to avoid the ``album
effect''~\cite{flexer2009album} which consists in a potential overestimation of
the performance of a system when a test set contains recordings from
the same albums as the training set. 
%For this reason we also included information
%about release groups of each recording in the development sets to help participants
%to avoid this effect when developing their systems.
The development sets additionally
include information about release groups of each recording 
%(release group MBID)
which may be useful for participants in order to avoid this effect when
developing their systems.

%The development datasets additionally include information about release groups
%to which the recordings belong (release group MBID). This information may be
%useful for participants in order to avoid the ``album
%effect''~\cite{flexer2009album}, which consists in potential overestimation of
%the performance of a classifier when a test set contains music recordings from
%the same albums as the training set. Our random split of recordings into
%development and test datasets ensures that such an effect does not occur. The
%approximate split ratio is 70\% to 15\% (another 15\% are reserved for further
%evaluation purposes).

\begin{table}
  \caption{Overview of the development datasets}
  \label{table:train_stats}
  \begin{tabular}{lcccc}
  \toprule
    Dataset & AllMusic & Discogs & Lastfm & Tagtraum \\
  \midrule
    Type & Explicit & Explicit & Tags & Tags \\
    Annotation level & Release & Release & Track & Track \\
  \midrule
    Tracks (recordings) & 1353213 & 904944 & 566710 & 486740 \\
    Release groups & 163654 & 118475 & 115161 & 69025 \\
  \midrule
    Genres & 21 & 15 & 30 & 31 \\
    Subgenres & 745 & 300 & 297 & 265 \\
    Genres/track & 1.33 & 1.37 & 1.14 & 1.13 \\
    Subgenres/track & 3.15 & 1.69 & 1.28 & 1.72 \\
  \bottomrule
\end{tabular}
\end{table}

\section{Submissions and Evaluation}

Participants are expected to submit predictions for both subtasks. If they only
want to work on the first subtask, they should submit the same predictions for
the second subtask. We allow only five evaluation runs (each run includes both
subtasks). Participants should report whether they used the whole development
dataset or only its part for every submission.

%\section{Evaluation Methodology}

The evaluation is carried out for each dataset separately. In particular, we
compute precision, recall and F-score as follows:

\begin{itemize}
\item Per recording, all labels.
\item Per recording, only genre labels.
\item Per recording, only subgenre labels.
\item Per label, all recordings.
\item Per genre label, all recordings.
\item Per subgenre label, all recordings.
\end{itemize}

The ground truth does not necessarily contain subgenre annotations for some
recordings. Therefore, only recordings containing subgenres are considered for
the evaluation on the subgenre level. An example can be found online in the
summaries of random
baselines.
\footnote{\url{https://multimediaeval.github.io/2017-AcousticBrainz-Genre-Task/baseline/}} 
We provide evaluation scripts for development purposes.\footnote{Accessible to registered participants.}

\section{Conclusions}
Bringing the AcousticBrainz Genre Task to MediaEval we hope to benefit from
contributions and expertise of a broader machine learning and multimedia retrieval
community. We refer to the MediaEval 2017 proceedings for further details on the
methods and results of teams participating in the task.

%\begin{figure}
%\includegraphics{fly}
%\caption{A sample black and white graphic.}
%\end{figure}

%\begin{figure}
%\includegraphics[height=1in, width=1in]{fly}
%\caption{A sample black and white graphic
%that has been resized with the \texttt{includegraphics} command.}
%\end{figure}


%\begin{figure*}
%\includegraphics{flies}
%\caption{A sample black and white graphic
%that needs to span two columns of text.}
%\end{figure*}
%
%
%\begin{figure}
%\includegraphics[height=1in, width=1in]{rosette}
%\caption{A sample black and white graphic that has
%been resized with the \texttt{includegraphics} command.}
%\end{figure}


\begin{acks} 

We thank all contributors to AcousticBrainz and Tagtraum. This research has
received funding from the European Union‚Äôs Horizon 2020 research and innovation
programme under grant agreement No 688382 (AudioCommons).

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\def\bibfont{\small} % comment this line for a smaller fontsize
\bibliography{acousticbrainz-genre-task-me17} 

\end{document}
