# Evaluation scripts

We provide scripts for evaluation of the task:
- a script to split development dataset into train and test (Python)
- a script to verify participants' submissions. This scripts verifies the format of the file
- a script for computing various evaluation metrics


### Prerequisites

Install R:

```
sudo apt-get install r-base
sudo apt-get install r-cran-plyr

sudo R
> install.packages("data.table")
q()
```


### Split development set into train and test

Example:

    python participant_split_data.py -s 80 -f acousticbrainz-mediaeval2017-discogs-train.tsv

This will create two files,

    acousticbrainz-mediaeval2017-discogs-train-train.tsv
    acousticbrainz-mediaeval2017-discogs-train-test.tsv

That is, the input filename plus the suffix `-train` or `-test` before the extension.

Use the `-s` option to set the percentage size of train set. If you do not set
this parameter a default value of 80% will be used.

It is recommended to use the `-f` option for release-group filtering. It
ensures there are no tracks from the same release group (i.e., album) in both
train and test sets to avoid any possible bias dues to the "album effect".
If you use the `-f` option the number of lines in the train/test set may not
exactly match the percentage size of `-s`, due to some albums having a different
number of recordings.

Use the `-m` option to set the minimum number of times a tag must appear in the
training set. Tags must also appear a minimum number of `m/2` times in the
training set.
If you do not set this parameter a default value of 20 will be used.
For example, if m is `20`, and a particular tag only appears in the test
split 8 times, this tag will be removed from both the test split _and_ the
train split.


### Verify the format of a participant run
The ```check.R``` script verifies the format of a submission run (the file with predictions generated by a participant's system). It ensures that there are no missing predictions for some recordings and that there are no predicted labels different from genre and subgenre labels expected to be in the ground truth.

```
Usage: Rscript check.R <source-name> <run-file>
where <source-name> is one of: 'allmusic', 'discogs', 'lastfm', 'tagtraum'.
```


### Evaluate

The `eval.R` script evaluates a participant run against a ground truth file. It computes a normalized confustion matrix, Precision, Recall and F-score as follows:

* Per track, with all labels.
* Per track, only with genres.
* Per track, only with subgenres.
* Per label, all labels.
* Per label, only genres.
* Per label, only subgenres.

It generates four files:

* `<run-file>.results_track` with the full matrix of results per track.
* `<run-file>.reuslts_label` with the full matrix of results per label.
* `<run-file>.results_summary` with a summary of results (mean and sd).
* `<run-file>.results_confusion` with the confusion matrix (actual by predicted). A true positive contributes 1 to the diagonal. A false positive contributes 1/fn to each false negative, so its total row contribution is 1.

```
Usage: Rscript eval.R <ground-truth-file> <run-file>
```








