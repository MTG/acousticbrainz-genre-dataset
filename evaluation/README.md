# Evaluation scripts

We provide scripts for evaluation of the task:

- a Python script to split development dataset into train and test.
- an R script to verify the format and contents of participants' submissions.
- an R script for computing various evaluation scores.

To install Python and R, please follow the instructions [here](https://www.python.org/) and [here](https://www.r-project.org/).

### Split development set into train and test

Script [`participant_split_data.py`](participant_split_data.py) creates train and test splits.
Example:

    python participant_split_data.py -s 80 -f acousticbrainz-mediaeval2017-discogs-train.tsv

This will create two files,

    acousticbrainz-mediaeval2017-discogs-train-train.tsv
    acousticbrainz-mediaeval2017-discogs-train-test.tsv

That is, the input filename plus the suffix `-train` or `-test` before the extension.

Use the `-s` option to set the percentage size of train set. If you do not set
this parameter a default value of 80% will be used.

It is recommended to use the `-f` option for release-group filtering. It
ensures there are no tracks from the same release group (i.e., album) in both
train and test sets to avoid any possible bias dues to the "album effect".
If you use the `-f` option the number of lines in the train/test set may not
exactly match the percentage size of `-s`, due to some albums having a different
number of recordings.

Use the `-m` option to set the minimum number of times a tag must appear in the
training set. Tags must also appear a minimum number of `m/2` times in the
training set.
If you do not set this parameter a default value of 20 will be used.
For example, if m is `20`, and a particular tag only appears in the test
split 8 times, this tag will be removed from both the test split _and_ the
train split.


### Verify the format of a participant run

Script [`check.R`](check.R) verifies the format of a submission run (the file with predictions generated by a participant's system). It ensures that there are no missing predictions for some recordings and that there are no predicted labels different from genre and subgenre labels expected to be in the ground truth.

```
Usage: Rscript check.R <source-name> <run-file>
where <source-name> is one of: 'allmusic', 'discogs', 'lastfm', 'tagtraum'.
```

The accompanying file [`check.data`](check.data) must be in the same directory as `check.R`.

### Evaluate

Script [`eval.R`](eval.R) evaluates a participant run against a ground truth file. It computes a normalized confustion matrix, Precision, Recall and F-score as follows:

* Per track, with all labels.
* Per track, only with genres.
* Per track, only with subgenres.
* Per label, all labels.
* Per label, only genres.
* Per label, only subgenres.

It generates four files:

* `<run-file>.results_track` with the full matrix of results per track.
* `<run-file>.reuslts_label` with the full matrix of results per label.
* `<run-file>.results_summary` with a summary of results (mean and sd).
* `<run-file>.results_confusion` with the confusion matrix (actual by predicted). A true positive contributes 1 to the diagonal. A false positive contributes 1/fn to each of the fn false negatives, so its total row contribution is 1.

```
Usage: Rscript eval.R <ground-truth-file> <run-file>
```








