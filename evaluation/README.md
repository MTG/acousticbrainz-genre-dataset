# Evaluation scripts

We provide scripts for evaluation of the task:
- a script to split development dataset into train and test (Python)
- a script to verify participants' submissions. This scripts verifies the format of the file
- a script for computing various evaluation metrics


### Prerequisites

Install R:

```
sudo apt-get install r-base
sudo apt-get install r-cran-plyr

sudo R
> install.packages("data.table")
q()
```


### Split development set into train and test

Example: 

```
python participant_split_data.py -s 80 -f acousticbrainz-mediaeval2017-discogs-train.tsv
```

It is recommended to use the ```-f``` option for release-group filtering. It ensures there are no tracks from the same release group (e.g., album) in both train and test sets to avoid any possible bias dues to the "album effect".

Use the ```-s``` option to set the percentage size of train set.


### Verify the format of a participant run
The ```check.R``` script verifies the format of a submission run (the file with predictions generated by a participant's system). It ensures that there are no missing predictions for some recordings and that there are no predicted labels different from genre and subgenre labels expected to be in the ground truth. 

```
Usage: Rscript check.R <source-name> <run-file>
where <source-name> is one of: 'allmusic', 'discogs', 'lastfm', 'tagtraum'.
```


### Evaluate

The `eval.R` script evaluates a participant run against a ground truth file. It computes a normalized confustion matrix, Precision, Recall and F-score as follows:

* Per track, with all labels.
* Per track, only with genres.
* Per track, only with subgenres.
* Per label, all labels.
* Per label, only genres.
* Per label, only subgenres.
    
It generates four files:

* `<run-file>.results_track` with the full matrix of results per track.
* `<run-file>.reuslts_label` with the full matrix of results per label.
* `<run-file>.results_summary` with a summary of results (mean and sd).
* `<run-file>.results_confusion` with the confusion matrix (actual by predicted). A true positive contributes 1 to the diagonal. A false positive contributes 1/fn to each false negative, so its total row contribution is 1.

```
Usage: Rscript eval.R <ground-truth-file> <run-file>
```








